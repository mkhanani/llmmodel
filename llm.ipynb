{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-13 12:14:48.458243: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, DataCollatorWithPadding\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['_id', 'id', 'name', 'description', 'price_per_unit', 'quantity',\n",
      "       'keywords'],\n",
      "      dtype='object')\n",
      "                        _id        id  \\\n",
      "0  6659a9e10db47e69c306e52f  AM-52030   \n",
      "1  6659a9e10db47e69c306e534  AM-95006   \n",
      "2  6659a9e10db47e69c306e530  AM-17741   \n",
      "3  6659a9e10db47e69c306e532  AM-18411   \n",
      "4  6659a9e10db47e69c306e533  AM-95012   \n",
      "\n",
      "                                                name  \\\n",
      "0                                      Chesse Slicer   \n",
      "1                Lacor Salad Spinner Manual 25 Liter   \n",
      "2              Lacor Confectionary Funnel With Stand   \n",
      "3  Paderno Abs Japanese Mandoline Slicer 15 x 35 ...   \n",
      "4                Lacor Salad Spinner Manual 12 Liter   \n",
      "\n",
      "                                         description price_per_unit quantity  \\\n",
      "0  Material: Stainless Steel, Dimension : 2.5 H x...            120        4   \n",
      "1  Manufacturer: Lacor Spain ,Item Code: 61425, M...         653.06       10   \n",
      "2  Manufacturer: Lacor Spain, Item Code: 67150, M...         244.88        6   \n",
      "3  Manufacturer; Paderno Italy, Item Code: 49753-...         441.43        9   \n",
      "4  Manufacturer: Lacor Spain, Item Code: 61420, M...         585.82        5   \n",
      "\n",
      "                                keywords  \n",
      "0            cheese slicer, cheese plane  \n",
      "1                          salad spinner  \n",
      "2  confectionery funnel, stand for above  \n",
      "3    japanese mandolin, vegetable slicer  \n",
      "4                          salad spinner  \n"
     ]
    }
   ],
   "source": [
    "# Connect to MongoDB and load data\n",
    "client = MongoClient('mongodb+srv://mkhanani69:AM123$Pro@cubicvision.1hhgged.mongodb.net/?retryWrites=true&w=majority&appName=CubicVision')\n",
    "db = client['horecastore']\n",
    "collection = db['products']\n",
    "data = list(collection.find())\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Check column names and inspect the first few rows\n",
    "print(df.columns)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jiwins Plastic Ingredient Bin White with Lid    3\n",
      "SCHNEIDER Ridged Baking Mould                   2\n",
      "Lacor Lid 40cm                                  2\n",
      "Lacor S/S Grid 650x530mm                        2\n",
      "Lacor S/S Grid 530x325mm                        2\n",
      "                                               ..\n",
      "Lacor Conical Mixing Bowl 30cm                  1\n",
      "Lacor Conical Mixing Bowl 20cm                  1\n",
      "Pujadas 1/2 Ball Reinforced Colander 19cm       1\n",
      "Lacor Conical Strainer With Fine Mesh 10cm      1\n",
      "Prestige Stainless Steel Rice Cooker 1.8L       1\n",
      "Name: name, Length: 375, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Ensure 'name' and 'description' columns are present\n",
    "if 'name' in df.columns and 'description' in df.columns:\n",
    "    # Text cleaning\n",
    "    df['description'] = df['description'].str.lower().str.replace('[^\\w\\s]', '', regex=True)\n",
    "\n",
    "    # Tokenization\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    df['tokens'] = df['description'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))\n",
    "\n",
    "    # Label encoding\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['label_encoded'] = label_encoder.fit_transform(df['name'])\n",
    "else:\n",
    "    print(\"Required columns 'name' and 'description' are not present in the DataFrame.\")\n",
    "\n",
    "# Check data distribution\n",
    "print(df['name'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for training\n",
    "class ProductDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data.iloc[idx]\n",
    "        return {\n",
    "            'input_ids': torch.tensor(item['tokens'], dtype=torch.long),\n",
    "            'label': torch.tensor(item['label_encoded'], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "dataset = ProductDataset(df)\n",
    "# Custom collate function\n",
    "def custom_collate_fn(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    labels = [item['label'] for item in batch]\n",
    "    \n",
    "    # Use DataCollatorWithPadding to pad input_ids\n",
    "    data_collator = DataCollatorWithPadding(tokenizer)\n",
    "    batch = data_collator([{'input_ids': input_id} for input_id in input_ids])\n",
    "    \n",
    "    # Add labels back to the batch\n",
    "    batch['labels'] = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    return batch\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/mustafakhanani/opt/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 6.011236667633057\n",
      "Epoch: 0, Loss: 5.995007514953613\n",
      "Epoch: 0, Loss: 6.052751541137695\n",
      "Epoch: 0, Loss: 6.018484592437744\n",
      "Epoch: 0, Loss: 5.926253795623779\n",
      "Epoch: 0, Loss: 5.914660453796387\n",
      "Epoch: 0, Loss: 5.952935218811035\n",
      "Epoch: 0, Loss: 5.836736679077148\n",
      "Epoch: 0, Loss: 6.008844375610352\n",
      "Epoch: 0, Loss: 5.9241790771484375\n",
      "Epoch: 0, Loss: 6.016955852508545\n",
      "Epoch: 0, Loss: 5.918779373168945\n",
      "Epoch: 0, Loss: 5.993082046508789\n",
      "Epoch: 0, Loss: 6.021120548248291\n",
      "Epoch: 0, Loss: 5.9600605964660645\n",
      "Epoch: 0, Loss: 6.0740156173706055\n",
      "Epoch: 0, Loss: 5.78887939453125\n",
      "Epoch: 0, Loss: 5.897466659545898\n",
      "Epoch: 0, Loss: 5.94154167175293\n",
      "Epoch: 0, Loss: 6.076204299926758\n",
      "Epoch: 0, Loss: 6.116046905517578\n",
      "Epoch: 0, Loss: 5.908572196960449\n",
      "Epoch: 0, Loss: 5.912426471710205\n",
      "Epoch: 0, Loss: 5.909814834594727\n",
      "Epoch: 0, Loss: 6.196063995361328\n",
      "Epoch: 1, Loss: 5.99150276184082\n",
      "Epoch: 1, Loss: 5.89846658706665\n",
      "Epoch: 1, Loss: 6.008439064025879\n",
      "Epoch: 1, Loss: 6.048862934112549\n",
      "Epoch: 1, Loss: 6.073078155517578\n",
      "Epoch: 1, Loss: 6.072360992431641\n",
      "Epoch: 1, Loss: 5.96516227722168\n",
      "Epoch: 1, Loss: 5.933320045471191\n",
      "Epoch: 1, Loss: 5.956542015075684\n",
      "Epoch: 1, Loss: 5.938970565795898\n",
      "Epoch: 1, Loss: 5.959445476531982\n",
      "Epoch: 1, Loss: 6.022034645080566\n",
      "Epoch: 1, Loss: 5.859861850738525\n",
      "Epoch: 1, Loss: 5.9639058113098145\n",
      "Epoch: 1, Loss: 5.89003849029541\n",
      "Epoch: 1, Loss: 5.911788463592529\n",
      "Epoch: 1, Loss: 5.977913856506348\n",
      "Epoch: 1, Loss: 6.002840995788574\n",
      "Epoch: 1, Loss: 6.026822090148926\n",
      "Epoch: 1, Loss: 5.901487350463867\n",
      "Epoch: 1, Loss: 5.956943035125732\n",
      "Epoch: 1, Loss: 6.047127723693848\n",
      "Epoch: 1, Loss: 5.939812660217285\n",
      "Epoch: 1, Loss: 5.987428188323975\n",
      "Epoch: 1, Loss: 5.740098476409912\n",
      "Epoch: 2, Loss: 5.985558032989502\n",
      "Epoch: 2, Loss: 5.963180065155029\n",
      "Epoch: 2, Loss: 5.909767150878906\n",
      "Epoch: 2, Loss: 5.910909652709961\n",
      "Epoch: 2, Loss: 5.877379417419434\n",
      "Epoch: 2, Loss: 6.022876262664795\n",
      "Epoch: 2, Loss: 5.98377799987793\n",
      "Epoch: 2, Loss: 5.8923749923706055\n",
      "Epoch: 2, Loss: 5.922121524810791\n",
      "Epoch: 2, Loss: 6.015264987945557\n",
      "Epoch: 2, Loss: 5.8689727783203125\n",
      "Epoch: 2, Loss: 5.873890399932861\n",
      "Epoch: 2, Loss: 6.028034687042236\n",
      "Epoch: 2, Loss: 5.903472423553467\n",
      "Epoch: 2, Loss: 5.943851470947266\n",
      "Epoch: 2, Loss: 5.887700080871582\n",
      "Epoch: 2, Loss: 5.877815246582031\n",
      "Epoch: 2, Loss: 5.974279403686523\n",
      "Epoch: 2, Loss: 5.8875017166137695\n",
      "Epoch: 2, Loss: 5.92737340927124\n",
      "Epoch: 2, Loss: 5.949586868286133\n",
      "Epoch: 2, Loss: 5.899091720581055\n",
      "Epoch: 2, Loss: 5.973900318145752\n",
      "Epoch: 2, Loss: 5.933465957641602\n",
      "Epoch: 2, Loss: 6.165193557739258\n",
      "Epoch: 3, Loss: 5.985414981842041\n",
      "Epoch: 3, Loss: 5.886518478393555\n",
      "Epoch: 3, Loss: 5.868729114532471\n",
      "Epoch: 3, Loss: 5.898180961608887\n",
      "Epoch: 3, Loss: 5.832280158996582\n",
      "Epoch: 3, Loss: 5.886728286743164\n",
      "Epoch: 3, Loss: 5.944698810577393\n",
      "Epoch: 3, Loss: 5.987811088562012\n",
      "Epoch: 3, Loss: 5.9335737228393555\n",
      "Epoch: 3, Loss: 5.913191795349121\n",
      "Epoch: 3, Loss: 5.921931743621826\n",
      "Epoch: 3, Loss: 5.934296131134033\n",
      "Epoch: 3, Loss: 5.974929332733154\n",
      "Epoch: 3, Loss: 5.890592098236084\n",
      "Epoch: 3, Loss: 5.934375286102295\n",
      "Epoch: 3, Loss: 5.976494789123535\n",
      "Epoch: 3, Loss: 6.014353275299072\n",
      "Epoch: 3, Loss: 6.0111565589904785\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder.classes_))\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(df['label_encoded']), y=df['label_encoded'])\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(10):  # Number of epochs\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids']\n",
    "        labels = batch['labels']\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'Epoch: {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids']\n",
    "        labels = batch['labels']\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs.logits\n",
    "        predictions.extend(torch.argmax(logits, dim=1).tolist())\n",
    "        true_labels.extend(labels.tolist())\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "# Detailed classification report\n",
    "print(classification_report(true_labels, predictions, target_names=label_encoder.classes_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
